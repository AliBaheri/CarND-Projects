{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "from math import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "tf.python.control_flow_ops = control_flow_ops\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Input, InputLayer\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import model_from_json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = './driving-data/'\n",
    "img_rows, img_cols = 160, 320\n",
    "crop_top = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(normalizer=255.0, vgg=False):\n",
    "    columns = ['center', 'left', 'right', 'steering', 'throttle', 'brake', 'speed']\n",
    "\n",
    "    print('Loading driving log ...')\n",
    "\n",
    "    driving_log = pd.read_csv(data_path+'driving_log.csv', names=columns)\n",
    "    num_rows = len(driving_log.index)\n",
    "\n",
    "    if vgg:\n",
    "        train_images = np.zeros((num_rows*3, 224, 224, 3))\n",
    "    else:\n",
    "        train_images = np.zeros((num_rows*3, img_rows, img_cols, 3))\n",
    "    train_steering = np.zeros(num_rows*3)\n",
    "\n",
    "    for index, row in tqdm_notebook(driving_log.iterrows(), unit=' rows', total=num_rows):\n",
    "        fname  = os.path.basename(row['center'])\n",
    "        fname1 = os.path.basename(row['left'])\n",
    "        fname2 = os.path.basename(row['right'])\n",
    "        # Normalized YUV\n",
    "        if vgg:\n",
    "            img = image.load_img(data_path+'IMG/'+fname, target_size=(224, 224))\n",
    "            img = image.img_to_array(img).astype(np.float32)\n",
    "            \n",
    "            img1 = image.load_img(data_path+'IMG/'+fname1, target_size=(224, 224))\n",
    "            img1 = image.img_to_array(img1).astype(np.float32)\n",
    "            \n",
    "            img2 = image.load_img(data_path+'IMG/'+fname2, target_size=(224, 224))\n",
    "            img2 = image.img_to_array(img2).astype(np.float32)\n",
    "        else:    \n",
    "            img  = imread(data_path+'IMG/'+fname, False, 'YCbCr').astype(np.float32)/normalizer\n",
    "            img1 = imread(data_path+'IMG/'+fname1, False, 'YCbCr').astype(np.float32)/normalizer\n",
    "            img2 = imread(data_path+'IMG/'+fname2, False, 'YCbCr').astype(np.float32)/normalizer\n",
    "\n",
    "        train_images[3*index] = img\n",
    "        train_images[3*index+1] = img1\n",
    "        train_images[3*index+2] = img2\n",
    "        \n",
    "        train_steering[3*index] = np.float32(row['steering'])\n",
    "        train_steering[3*index+1] = np.float32(row['steering'])+2./25.\n",
    "        train_steering[3*index+2] = np.float32(row['steering'])-2./25.\n",
    "        \n",
    "\n",
    "    print('Loaded', num_rows, 'rows.')\n",
    "    return train_images, train_steering\n",
    "\n",
    "def image_generator(driving_log, normalizer=255.0, vgg=False, steering_shift=3., steering_max=25.):\n",
    "    \n",
    "    for index, row in driving_log.iterrows():\n",
    "        fname  = os.path.basename(row['center'])\n",
    "        fname1 = os.path.basename(row['left'])\n",
    "        fname2 = os.path.basename(row['right'])\n",
    "        \n",
    "        if vgg:\n",
    "            img = image.load_img(data_path+'IMG/'+fname, target_size=(224, 224))\n",
    "            img = image.img_to_array(img).astype(np.float32)\n",
    "            \n",
    "            img1 = image.load_img(data_path+'IMG/'+fname1, target_size=(224, 224))\n",
    "            img1 = image.img_to_array(img1).astype(np.float32)\n",
    "            \n",
    "            img2 = image.load_img(data_path+'IMG/'+fname2, target_size=(224, 224))\n",
    "            img2 = image.img_to_array(img2).astype(np.float32)\n",
    "        else:\n",
    "            # Normalized YUV\n",
    "            img  = imread(data_path+'IMG/'+fname, False, 'YCbCr').astype(np.float32)\n",
    "            img1 = imread(data_path+'IMG/'+fname1, False, 'YCbCr').astype(np.float32)\n",
    "            img2 = imread(data_path+'IMG/'+fname2, False, 'YCbCr').astype(np.float32)\n",
    "        \n",
    "        yield img, np.float32(row['steering'])\n",
    "#         yield np.fliplr(img), -np.float32(row['steering'])\n",
    "        \n",
    "        yield img1, np.float32(row['steering'])+steering_shift/steering_max\n",
    "#         yield np.fliplr(img1), -(np.float32(row['steering'])+steering_shift/steering_max)\n",
    "        \n",
    "        yield img2, np.float32(row['steering'])-steering_shift/steering_max\n",
    "#         yield np.fliplr(img2), -(np.float32(row['steering'])-steering_shift/steering_max)\n",
    "        \n",
    "def normalize_color(image_data):\n",
    "    a = -0.5\n",
    "    b = +0.5\n",
    "    \n",
    "    Xmin = 0.0\n",
    "    Xmax = 255.0\n",
    "\n",
    "    norm_img = np.empty_like(image_data, dtype=np.float32)\n",
    "\n",
    "    norm_img = a + (image_data - Xmin)*(b-a)/(Xmax - Xmin)\n",
    "    return norm_img\n",
    "\n",
    "def batch_generator(driving_log, batch_size=32, *args, vgg=False, **kwargs):\n",
    "    num_rows = len(driving_log.index)\n",
    "    train_images = np.zeros((batch_size, img_rows, img_cols, 3))\n",
    "    train_steering = np.zeros(batch_size)\n",
    "    ctr = None\n",
    "    while 1:        \n",
    "        for j in range(batch_size):\n",
    "            # Reset generator if over bounds\n",
    "            if ctr is None or ctr >= num_rows:\n",
    "                ctr = 0\n",
    "                images = image_generator(driving_log, *args, **kwargs)\n",
    "            train_images[j], train_steering[j] = next(images)\n",
    "            ctr += 1\n",
    "        yield normalize_color(train_images), (train_steering+1.)/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = ['center', 'left', 'right', 'steering', 'throttle', 'brake', 'speed']\n",
    "driving_log = pd.read_csv(data_path+'driving_log.csv', names=columns)\n",
    "num_rows = len(driving_log.index)\n",
    "train_data = batch_generator(driving_log.iloc[:-1000], batch_size=50)\n",
    "val_data = batch_generator(driving_log.iloc[-1000:], batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model_name = 'model_1'\n",
    "# with open(model_name+'.json', 'r') as jfile:\n",
    "#     # model = model_from_json(json.load(jfile))\n",
    "#     model = model_from_json(jfile.read())\n",
    "\n",
    "# model.compile(\"adam\", \"mse\")\n",
    "# weights_file = model_name+'.h5'\n",
    "# model.load_weights(weights_file)\n",
    "# model.summary()\n",
    "# print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(24, 5, 5, border_mode='same',\n",
    "                        input_shape=(img_rows, img_cols, 3), subsample=(2,2)))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('relu', name='Conv1'))\n",
    "\n",
    "model.add(Convolution2D(36, 5, 5, subsample=(2,2)))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('relu', name='Conv2'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(48, 5, 5))#, subsample=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu', name='Conv3'))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu', name='Conv4'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "# model.add(Convolution2D(64, 3, 3))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu', name='Conv5'))\n",
    "# model.add(MaxPooling2D((2,2)))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1164))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('relu', name='FC1'))\n",
    "model.add(Dense(100))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('relu', name='FC2'))\n",
    "model.add(Dense(50))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('relu', name='FC3'))\n",
    "model.add(Dense(10))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu', name='FC4'))\n",
    "model.add(Dense(1, activation='relu', name='output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=1e-4)\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 63s - loss: 0.0024 - val_loss: 0.0332\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 60s - loss: 0.0022 - val_loss: 0.0154\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 59s - loss: 0.0019 - val_loss: 0.0133\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 59s - loss: 0.0020 - val_loss: 0.0069\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 59s - loss: 0.0019 - val_loss: 0.0096\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 59s - loss: 0.0016 - val_loss: 0.0098\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 59s - loss: 0.0016 - val_loss: 0.0050\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 59s - loss: 0.0016 - val_loss: 0.0128\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 58s - loss: 0.0016 - val_loss: 0.0355\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 59s - loss: 0.0012 - val_loss: 0.0051\n"
     ]
    }
   ],
   "source": [
    "# h = model.fit(X_train, y_train, batch_size=32, nb_epoch=5, verbose=1)\n",
    "h = model.fit_generator(train_data, validation_data = val_data,\n",
    "                        samples_per_epoch = 10000,\n",
    "                        nb_val_samples = 1000,\n",
    "                        nb_epoch=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0011621094308793545"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(val_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "model_name = 'model_4b'\n",
    "with open(model_name+'.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(model_name+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "vgg = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "model2 = Model(input=vgg.input, output=vgg.get_layer('block3_pool').output)\n",
    "    \n",
    "img_path = 'driving-data/IMG/center_2016_11_27_23_18_18_813.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "x = image.img_to_array(img)\n",
    "\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "block4_pool_features = model2.predict(x)\n",
    "print(block4_pool_features.shape)\n",
    "\n",
    "# plt.figure()\n",
    "# for i in range(16):\n",
    "#     plt.subplot(4, 4, i+1)\n",
    "#     plt.imshow(block4_pool_features[0,:,:,16+i])\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.layers[0] = InputLayer(input_shape=(224,224,3), input_dtype=np.float32)\n",
    "\n",
    "for l in model2.layers:\n",
    "    l.trainable=False\n",
    "    \n",
    "model = Sequential(model2.layers)#, input_shape=(12, 40, 256))  # 20,40,256\n",
    "model.add(Convolution2D(24, 5, 5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Convolution2D(36, 5, 5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(48, 5, 5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1154))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(50))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X1_train, y1_train = load_data(normalizer=1.0, vgg=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X1_train, y1_train, test_size=0.05, random_state=0xdeadbeef)\n",
    "del X1_train, y1_train\n",
    "X2_train = preprocess_input(X_train)\n",
    "X2_val = preprocess_input(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=1e-3)\n",
    "model.compile(optimizer=opt, loss='mae')\n",
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = model.fit(X2_train, y_train, batch_size=35, nb_epoch=10,\n",
    "    validation_data=(X2_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "model_name = 'vgg_01'\n",
    "with open(model_name+'.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(model_name+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predict(X2_train[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
